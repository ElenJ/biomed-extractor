# Model Selection Process and Rationale

**Project:** Biomedical LLM Information Extraction Tool  
**Version:** 0.1  
**Author:** Elena Jolkver
**Date:** 25.09.2025



## 1. Introduction

This document details the process and rationale behind the selection of NLP models used for information extraction and summarization in the Biomedical LLM Information Extraction Tool. The goal is to ensure effective, accurate extraction of clinical trial key elements (such as Population, Intervention, Comparator, and Outcomes—PICO) from complex, domain-specific text.



## 2. Model Selection Criteria

To choose the appropriate models, the following criteria were considered:

- **Domain Adaptation:** Preference for models pre-trained or fine-tuned on biomedical or clinical corpora for higher accuracy in life science language (e.g. PubMed, MIMIC-III).
- **Task Suitability:** Support for Named Entity Recognition (NER), sequence labeling, and abstractive summarization.
- **Scalability and Deployment:** Ability to run inference efficiently on CPUs or modest GPUs with reasonable latency, supporting Dockerized/local operation.
- **Community and Documentation:** Open-source models with active support and robust documentation.
- **Extensibility:** Models that are easily updatable or replaceable as research progresses or needs change.
- **Privacy:** Option to operate entirely offline, without mandatory API calls to external services, for use in high-compliance environments.



## 3. MoBiomedical/Clinical Pre-trained Modelsdels Considered


| Focus on      | Model                       | Availability In Huggingface                                                                             | Model Size | Parameters      | Training -   ResourceRequirementsRAMVRAMDockerCompatibility                                                                                                                                              | Inference -   ResourceRequirementsRAMVRAMDockerCompatibility                                           | SupportForLocalOfflineBatchInference | LicensingConsiderations                              | Reference                                                                | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Decision Support*                                                                       |
|---------------|-----------------------------|---------------------------------------------------------------------------------------------------------|------------|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------|------------------------------------------------------|--------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|
| NER           | BioBERT                     | https://huggingface.co/dmis-lab/biobert-v1.1                                                            | Base       | 110M            | GPU: At least one NVIDIA GPU   with 12GB+ VRAM (e.g., Tesla V100, RTX 2080 Ti)      RAM: 16–32 GB system memory recommended      Storage: ~4–6 GB for model weights and biomedical corpora (PubMed, PMC) | Moderate, runs on CPU (Intel   i5/i7, 8–16 GB RAM, GPU (optional, NVIDIA GTX 1060 (6GB)), ≥ 6 GB VRAM) | Yes                                  | Apache 2.0                                           | https://academic.oup.com/bioinformatics/article/36/4/1234/5566506        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| NER           | ClinicalBERT                | https://huggingface.co/medicalai/ClinicalBERT;   https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT | Base       | 110M            | GeForce   GTX TITAN X 12 GB GPU                                                                                                                                                                          |                                                                                                        | Yes                                  | Apache   2.0                                         | https://arxiv.org/pdf/1904.05342;   https://arxiv.org/pdf/1904.03323     | ClinicalBERT was trained on the MIMIC-III dataset that   includes inpatient documentation on patients hospitalized in intensive care   units. Thus, for the purpose of PICOS extraction, BioBERT might be more   suitable, as BioBERT's Terminology from this domain may be more commonly   found in scientific literature on which BioBERT was trained. BioBERT, which   is initialized with weights from the original BERT, and then pre-trained on   PubMed abstracts and PubMed Central full-text articles . Although   ClinicalBERT has advantages in handling specialized clinical terms, it lacks   training data for a wide range of medical literature, and thus is slightly   inferior to BioBERT and BlueBERT in generalization ability. |                                                                                         |
| NER           | SciBERT                     | https://huggingface.co/allenai/scibert_scivocab_uncased                                                 | Base       | 110M            |                                                                                                                                                                                                          |                                                                                                        | Yes                                  | Apache 2.0                                           | https://arxiv.org/pdf/1903.10676;   https://www.arxiv.org/pdf/2412.08255 | SciBERT focuses on scientific   literature, especially literature in the fields of biomedicine and computer   science, which gives it an advantage in academic papers and research data   analysis. SciBERT is mainly pre-trained based on literature in the scientific   field and performs relatively well in biomedical literature. However, its   training data is not limited to the medical field but also involves other   disciplines, so SciBERT's performance in medical named entity recognition   tasks is slightly limited                                                                                                                                                                                                             |                                                                                         |
| NER           | BlueBERT                    | bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12                                                    | Base       | 110M            |                                                                                                                                                                                                          |                                                                                                        | Yes                                  | Apache   2.0                                         | https://www.arxiv.org/pdf/2412.08255                                     | BlueBERT   is a model that combines PubMed and MIMICIII datasets. By pre-training on   scientific literature and medical records, BlueBERT can better adapt to texts   containing professional terms and patient data. BlueBERT's precision and F1   score are second to BioBERT, at 87.3% and 85.0% respectively, indicating that   it has a good performance in the task of medical named entity recognition   (https://www.arxiv.org/pdf/2412.08255). This may be      because BlueBERT's pre-training data, while covering a wide range, is not   as refined as BioBERT in the biomedical field, especially in the fine-grained   recognition of named entities.                                                                                |                                                                                         |
| NER           | MSR BiomedBERT (PubMedBERT) | https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext                    | Base       | 110M            |                                                                                                                                                                                                          |                                                                                                        | Yes                                  | Apache 2.0                                           | https://arxiv.org/pdf/2007.15779                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | among base models, this one is   also good, 2nd after BioLinkBert in macroaverage       |
| NER           | BioMegatron                 | https://huggingface.co/EMBO/BioMegatron345mUncased                                                      | Large      | 345M            |                                                                                                                                                                                                          |                                                                                                        | Yes                                  | Apache   2.0                                         |                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| NER           | Clinical ModernBERT         | https://huggingface.co/Simonlee711/Clinical_ModernBERT                                                  | Base       | 137M            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      |                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| NER           | MedFound                    | https://huggingface.co/medicalai/MedFound-7B                                                            | Large      | 7   billion     |                                                                                                                                                                                                          |                                                                                                        |                                      | only for   research, no clinical or commercial usage | https://www.nature.com/articles/s41591-024-03416-6                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| NER           | BioRoBERTa                  | https://huggingface.co/minhpqn/bio_roberta-base_pubmed                                                  | Base       | 125M            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      |                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| NER           | BioLinkBERT                 | https://huggingface.co/michiyasunaga/BioLinkBERT-large                                                  | Large      | 340M parameters |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://aclanthology.org/2021.bionlp-1.24.pdf                            | top performer on bio tasks macro average                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | top performer on bio tasks macro average                                                |
| NER           | BioLinkBERT-Base            | https://huggingface.co/michiyasunaga/BioLinkBERT-base                                                   | Base       | 110M            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      |                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | best base model for macroaverage; best base model for QA                                |
| NER           | BioM-ELECTRA-Base           | https://huggingface.co/sultan/BioM-ELECTRA-Base-Discriminator                                           | Base       | 110M            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      |                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | top performer on PICO task                                                              |
| NER           | BioELECTRA-PICO             | https://huggingface.co/kamalkraj/BioELECTRA-PICO                                                        | Base       | 110M            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://aclanthology.org/2021.bionlp-1.16/                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | specifically for PICO?                                                                  |
| NER           | BioDistilBERT               | nlpie/bio-distilbert-cased                                                                              | Small      | 65M             |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2209.03182                                         | Pretrained on PubMed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                         |
| NER           | BioTinyBERT                 | nlpie/bio-tinybert                                                                                      | Tiny       | 15M             |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2209.03182                                         | Pretrained on PubMed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                         |
| NER           | BioMobileBERT               | nlpie/bio-mobilebert                                                                                    | Tiny       | 25M             |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2209.03182                                         | Pretrained on PubMed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | best model for NER among the tiny models                                                |
| NER           | DistilBioBERT               | nlpie/distil-biobert                                                                                    | Small      | 65M             |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2209.03182                                         | Distilled from BioBERT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                         |
| NER           | TinyBioBERT                 | nlpie/tiny-biobert                                                                                      | Tiny       | 15M             |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2209.03182                                         | Distilled from BioBERT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                         |
| NER           | CompactBioBERT              | nlpie/compact-biobert                                                                                   | Small      | 65M             |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2209.03182                                         | Distilled from BioBERT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | best model among small                                                                  |
| Summarization | BioGPT                      | https://huggingface.co/microsoft/biogpt                                                                 | Base       | 137M            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2210.10341                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | model outperfroms BioLinkBERT for QA and GPT for biomedical text   generation relevance |
| Summarization | DRAGON                      | https://github.com/michiyasunaga/dragon                                                                 | Large      | 360M            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2210.09338                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| Summarization | PubMedGPT (BioMedLM)        | https://huggingface.co/stanford-crfm/BioMedLM                                                           | Large      | 2.7B            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| Summarization | Med-PaLM 2                  | model not open-sourced                                                                                  | Large      | 340B            |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://www.nature.com/articles/s41591-024-03423-7.pdf                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |
| Summarization | Med-Gemini                  | model not open-sourced                                                                                  | Large      | 1.8B-540B       |                                                                                                                                                                                                          |                                                                                                        |                                      |                                                      | https://arxiv.org/pdf/2404.18416                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                         |

These models are pre-trained or further pre-trained on large biomedical literature, clinical notes, or PubMed abstracts, which makes them ideal for accurate NER and relation extraction in clinical trial documentation.

*Notes and Decision Support: based on the [BLURB benchmark](https://microsoft.github.io/BLURB/leaderboard.html) for biomedical NLP tasks.



## 4. Selected Models

Transformers" paper](https://arxiv.org/abs/2104.04473).
Overall comparison of current biomedical models is benchmarked [here](https://microsoft.github.io/BLURB/leaderboard.html). This benchmark is described in detail in the [BLURB paper](https://arxiv.org/pdf/2007.15779). Distilled models are also available for lightweight applications and described in the paper ["On the Effectiveness of Compact Biomedical Transformers"](https://arxiv.org/pdf/2209.03182).

### 4.1 Information Extraction (PICO Elements & Medical NER)

**Primary Model:**  
- **BioM-ELECTRA-Base** 
- Rationale:  
  - Outstanding performance in PICO element extraction (top macro and PICO F1).
  - Moderate resource requirements (CPU: ≥8GB RAM, GPU: ≥6GB VRAM).
  - Justification: Best model among base-models for PICO extraction, making it the best suited for extracting structured metadata from clinicaltrials.gov records. Macro-average and F1 scores consistently lead among contemporary models.

**Fallback/Secondary Models:**  
- **BioLinkBERT-Base**
  - High performer for general biomedical NER and other tasks (RE, **QA**).
  - Moderate requirements, broad entity extraction capability, suitable for use cases beyond just PICO.

- **BioBERT-Base**
  - classical bio-model
  - in training on PICO dataset, received high F1 score 

  
Epoch	Training Loss	Validation Loss	F1
1	0.236000	0.042140	0.915711
2	0.028500	0.007056	0.980917
3	0.005900	0.002192	0.995387


**Lightweight/Constrained Hardware Option:**

- BioMobileBERT or CompactBioBERT
  - Quantized/distilled versions for inference on lower-memory hardware
  - minor relative performance drop.
 
After training: nlpie/compact-biobert yielded higher F1 on validation dataset (prefer to BioMobileBERT)

CompactBiobert
Epoch	Training Loss	Validation Loss	F1
1	0.284500	0.064327	0.882915
2	0.045900	0.011879	0.970512
3	0.012500	0.005575	0.986721

BioMobileBERT
Epoch	Training Loss	Validation Loss	F1
1	0.323100	0.099416	0.790831
2	0.068300	0.022163	0.946324
3	0.021400	0.008656	0.979878



### 4.2 Summarization

**Primary Model:**  
- **BioGPT**
- Rationale:  
  - BioGPT outperformed  previous best performance for QA obtained by BioLinkBERT, achieving a new state-of-the-art on this task [publication, table 5](https://arxiv.org/pdf/2210.10341)
  - BioGPT showed higher accuracy in text generation related to biomedical literature, making it suitable for generating structured summaries from clinical trial abstracts.
  - downside: BioGPT is a decoder model only, so it is nout suitable for summarization, requiring encoder-decoder architecture.

  **Fallback/Secondary Models:**  
- **BioLinkBERT-Base** or **BioBERT**
  - High performer for general biomedical QA and other tasks (RE, NER).
  - Moderate requirements, broad entity extraction capability, suitable for multiple use cases, simplifying pipeline by using the same model for both extraction and summarization.
  - BioBERT is a well-known model for biomedical text, but BioLinkBERT is more recent and has shown better performance in benchmarks.

**Lightweight/Constrained Hardware Option:**

- BioMobileBERT or CompactBioBERT
  - Quantized/distilled versions for inference on lower-memory hardware
  - minor relative performance drop.

**Note:**
Despite the research upfront of the project, the selected models were not usable for summarization, so the summarization model was changed to a different one, which is not listed in the table above. Alternatives could have been the abstractive summary model L-macc/autotrain-Biomedical_sc_summ-1217846148, which is an encoder-decoder model suitable for summarization tasks, or the extractive summarization model NotXia/pubmedbert-bio-ext-summ, which is also an encoder-decoder model. Both models are available on Huggingface and can be used for summarization tasks. Due to size constraints of L-macc/autotrain-Biomedical_sc_summ-1217846148 as well as its time usage, extractive summarization was preferred for this task. However, rather than referring to the extractive summarization LLM, the conventional TextRank algorithm was used for summarization, which is a well-known algorithm for extractive summarization tasks. The TextRank algorithm is implemented in the `sumy` library, which is a Python library for text summarization. The TextRank algorithm is based on the PageRank algorithm and is suitable for extractive summarization tasks. It works by ranking sentences in a document based on their importance and relevance to the overall content of the document. The TextRank algorithm is a good choice for summarization tasks where the goal is to extract the most important sentences from a document without generating new text.  

## 5. Model Evaluation and Decision Process

- Conducted small-scale comparative tests on sample ClinicalTrials.gov documents.
- Benchmarked for extraction quality (precision, recall of PICO elements) and summarization coherence/readability.
- Considered ease of integration, compatibility with Streamlit/fast batch inference, and Dockerization.
- Documented performance metrics and qualitative feedback from test users (see `evaluation.md` for details).



## 6. Model Deployment Considerations

- Self-finetuned models can be loaded and run within a self-contained Docker image, with no external API dependency for inference.
- Model weights are downloaded from huggingface on first build/deployment if not included in the image.
- Distilled variants considered for edge cases (low-resource deployment).



## 7. Future Directions

- Explore BioELECTRA-large and BioLinkBERT-large as resources permit.
- Provide user-selectable model options in the interface for specialized use cases.
- Continuously monitor the biomedical NLP literature to update models as standards evolve.
- Develop summarization model



## 8. References

- [BioBERT](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)
- [Distilled models](https://arxiv.org/pdf/2209.03182)
- [BioLinkBERT](https://aclanthology.org/2021.bionlp-1.24.pdf)
- [BioM-ELECTRA-Base](https://www.aclweb.org/anthology/2021.bionlp-1.24)
- [BioGPT](https://arxiv.org/pdf/2210.10341)
- [Huggingface Model Hub](https://huggingface.co/models)

